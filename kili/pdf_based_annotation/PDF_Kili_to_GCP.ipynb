{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import uuid\n",
        "import json\n",
        "import base64\n",
        "import requests\n",
        "import copy\n",
        "\n",
        "# Define the PDF file path\n",
        "pdf_path = \"/content/Invoices.pdf\"\n",
        "\n",
        "# Function to send PDF for OCR processing\n",
        "def get_ocr_data(pdf_path):\n",
        "    # Google Document AI endpoint\n",
        "    api_url = \"https://us-documentai.googleapis.com/v1/projects/205398200267/locations/us/processors/2dde9d14469e70c9:process\"\n",
        "\n",
        "    # Read the PDF content and encode it in base64\n",
        "    with open(pdf_path, \"rb\") as pdf_file:\n",
        "        pdf_content = pdf_file.read()\n",
        "\n",
        "    # Convert PDF to base64 (this is required for most APIs)\n",
        "    encoded_pdf = base64.b64encode(pdf_content).decode(\"utf-8\")\n",
        "\n",
        "    # API request payload (adjusted structure for Document AI)\n",
        "    payload = {\n",
        "        \"skipHumanReview\": True,\n",
        "        \"inlineDocument\": {\n",
        "            \"mimeType\": \"application/pdf\",\n",
        "            \"content\": encoded_pdf\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # Make the API call (ensure you have an API key or OAuth token)\n",
        "    headers = {\"Authorization\": \"Bearer ya29.a0ARW5m77a9xCdpVH00-GpRh5PbqJMtJrEUIOatYVaoGZWl64wLmfPZ5400jtA3yqr-5frSaE0N9lifir1UzbDkXviKCVq6Nj6V86EdzfIQ_vS1cedWl-drw_fn48DQDw_MlrmAIzLCr8Tv9howmONQUkwjhvc5Cd_9l3mutzr3ACYJzkIaCgYKAXUSARMSFQHGX2Mi9rfaw6NRcaajB77iZdv7uw0183\"}\n",
        "    response = requests.post(api_url, headers=headers, json=payload)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        return response.json()\n",
        "    else:\n",
        "        raise Exception(f\"Error: {response.status_code}, {response.text}\")\n",
        "\n",
        "# Call the OCR function and get the data\n",
        "ocr_response = get_ocr_data(pdf_path)\n",
        "\n",
        "def to_camel_case(snake_str):\n",
        "    \"\"\"Convert snake_case to camelCase.\"\"\"\n",
        "    components = snake_str.lower().split('_')\n",
        "    return components[0] + ''.join(x.title() for x in components[1:])\n",
        "\n",
        "def find_word_indices(content, word):\n",
        "    \"\"\"Find the start and end indices of a word in the digitized content, ignoring case differences.\"\"\"\n",
        "    content_lower = content.lower()\n",
        "    word_lower = word.lower()\n",
        "    start_index = content_lower.find(word_lower)\n",
        "    if start_index == -1:\n",
        "        return None  # Word not found\n",
        "    end_index = start_index + len(word)\n",
        "    return start_index, end_index\n",
        "\n",
        "def map_kili_to_gcp(kili_response, digitized_content, ocr_response):\n",
        "    \"\"\"Map Kili annotations to GCP-compatible JSON format.\"\"\"\n",
        "    gcp_response = {\n",
        "        \"entities\": [],\n",
        "        \"pages\": [],\n",
        "        \"shardInfo\": {\n",
        "            \"shardCount\": \"1\"\n",
        "        },\n",
        "        \"text\": ocr_response['document']['text'],\n",
        "        \"uri\": \"\"  # Assuming this is a placeholder, adjust if necessary\n",
        "    }\n",
        "\n",
        "    # Add pages to GCP response\n",
        "    for page in ocr_response['document']['pages']:\n",
        "        gcp_response[\"pages\"].append(page)\n",
        "\n",
        "    # Debugging: Check structure of Kili JSON\n",
        "    if kili_response and isinstance(kili_response, list):\n",
        "        kili_json_response = kili_response[0].get(\"latestLabel\", {}).get(\"jsonResponse\", {})\n",
        "        # print(\"Keys in jsonResponse:\", list(kili_json_response.keys()))  # Debugging\n",
        "\n",
        "        for job_key, job_value in kili_json_response.items():\n",
        "            if \"annotations\" in job_value:\n",
        "                annotations = job_value[\"annotations\"]\n",
        "                # print(f\"Found {len(annotations)} annotations under {job_key}\")  # Debugging\n",
        "\n",
        "                for annotation_entry in annotations:\n",
        "                    for annotation in annotation_entry.get(\"annotations\", []):\n",
        "                        # Debugging: Print each annotation\n",
        "                        # print(f\"Annotation: {annotation}\")\n",
        "\n",
        "                        # Extract mention text and category\n",
        "                        mention_text = annotation_entry.get(\"content\", \"\").upper()\n",
        "                        category = annotation_entry.get(\"categories\", [{}])[0].get(\"name\", \"UNKNOWN\")\n",
        "\n",
        "                        # Convert category name to camelCase for GCP \"type\"\n",
        "                        entity_type = to_camel_case(category)\n",
        "\n",
        "                        # Generate a unique ID for each entity\n",
        "                        entity_id = str(uuid.uuid4())[:16]\n",
        "\n",
        "                        # Find the start and end indices based on the digitized content\n",
        "                        indices = find_word_indices(digitized_content, mention_text)\n",
        "\n",
        "                        if indices is None:\n",
        "                            print(f\"Warning: '{mention_text}' not found in digitized content.\")\n",
        "                            start_index, end_index = \"\", \"\"\n",
        "                        else:\n",
        "                            start_index, end_index = indices\n",
        "\n",
        "                        # Map boundingPoly and normalizedVertices\n",
        "                        bounding_poly = annotation.get(\"boundingPoly\",[{}])[0].get(\"normalizedVertices\",[{}])\n",
        "\n",
        "                        gcp_entity = {\n",
        "                            \"confidence\":1,\n",
        "                            \"id\": entity_id,\n",
        "                            \"mentionText\": mention_text,\n",
        "                            \"pageAnchor\": {\n",
        "                                \"pageRefs\": [\n",
        "                                    {\n",
        "                                        \"boundingPoly\": {\n",
        "                                            \"normalizedVertices\": bounding_poly\n",
        "                                        },\n",
        "                                        \"layoutType\": \"VISUAL_ELEMENT\"\n",
        "                                    }\n",
        "                                ]\n",
        "                            },\n",
        "                            \"textAnchor\": {\n",
        "                                \"content\": mention_text + \"\\n\",\n",
        "                                \"textSegments\": [\n",
        "                                    {\n",
        "                                        \"endIndex\": str(end_index) if end_index else \"\",\n",
        "                                        \"startIndex\": str(start_index) if start_index else \"\"\n",
        "                                    }\n",
        "                                ]\n",
        "                            },\n",
        "                            \"type\": entity_type\n",
        "                        }\n",
        "                        gcp_response[\"entities\"].append(gcp_entity)\n",
        "\n",
        "    return gcp_response\n",
        "\n",
        "def flatten_normalized_vertices(data):\n",
        "    \"\"\"\n",
        "    Recursively processes the JSON to flatten nested normalizedVertices arrays.\n",
        "    \"\"\"\n",
        "    if isinstance(data, list):\n",
        "        return [flatten_normalized_vertices(item) for item in data]\n",
        "\n",
        "    if isinstance(data, dict):\n",
        "        new_dict = {}\n",
        "        for key, value in data.items():\n",
        "            if key == 'normalizedVertices' and isinstance(value, list) and len(value) > 0:\n",
        "                # Remove one level of nesting if it exists\n",
        "                new_dict[key] = value[0] if isinstance(value[0], list) else value\n",
        "            else:\n",
        "                new_dict[key] = flatten_normalized_vertices(value)\n",
        "        return new_dict\n",
        "\n",
        "    return data\n",
        "\n",
        "def process_kili_json(kili_json):\n",
        "    # Create a deep copy to avoid modifying the original data\n",
        "    modified_json = copy.deepcopy(kili_json)\n",
        "    return flatten_normalized_vertices(modified_json)\n",
        "\n",
        "# Open the JSON file\n",
        "with open('Kili_invoice_response.json', 'r') as file:\n",
        "    # Load the JSON content\n",
        "    kili_json = json.load(file)\n",
        "\n",
        "digitized_content = ocr_response['document']['text']\n",
        "\n",
        "# Example usage\n",
        "# Assuming kili_json is your input JSON\n",
        "modified_data = process_kili_json(kili_json)\n",
        "\n",
        "# Map Kili annotations to GCP format\n",
        "gcp_json = map_kili_to_gcp(modified_data, digitized_content,ocr_response)\n",
        "\n",
        "# Save to a JSON file\n",
        "output_file = \"python_pdf_kili_to_gcp_invoice_11.json\"\n",
        "with open(output_file, \"w\", encoding=\"utf-8\") as file:\n",
        "    json.dump(gcp_json, file, ensure_ascii=False, indent=4)\n",
        "\n",
        "print(f\"GCP JSON saved to {output_file}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zZSUV94rpEqG",
        "outputId": "93e3ae5a-452d-4646-87b9-d3227a5783e1"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GCP JSON saved to python_pdf_kili_to_gcp_invoice_11.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Resume with single line"
      ],
      "metadata": {
        "id": "0yMsZNhTCkBL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import uuid\n",
        "import json\n",
        "import base64\n",
        "import requests\n",
        "import copy\n",
        "\n",
        "# Define the PDF file path\n",
        "pdf_path = \"/content/CriminalJusticeResumedocx.pdf\"\n",
        "\n",
        "# Function to send PDF for OCR processing\n",
        "def get_ocr_data(pdf_path):\n",
        "    # Google Document AI endpoint\n",
        "    api_url = \"https://us-documentai.googleapis.com/v1/projects/205398200267/locations/us/processors/2dde9d14469e70c9:process\"\n",
        "\n",
        "    # Read the PDF content and encode it in base64\n",
        "    with open(pdf_path, \"rb\") as pdf_file:\n",
        "        pdf_content = pdf_file.read()\n",
        "\n",
        "    # Convert PDF to base64 (this is required for most APIs)\n",
        "    encoded_pdf = base64.b64encode(pdf_content).decode(\"utf-8\")\n",
        "\n",
        "    # API request payload (adjusted structure for Document AI)\n",
        "    payload = {\n",
        "        \"skipHumanReview\": True,\n",
        "        \"inlineDocument\": {\n",
        "            \"mimeType\": \"application/pdf\",\n",
        "            \"content\": encoded_pdf\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # Make the API call (ensure you have an API key or OAuth token)\n",
        "    headers = {\"Authorization\": \"Bearer ya29.a0ARW5m77a9xCdpVH00-GpRh5PbqJMtJrEUIOatYVaoGZWl64wLmfPZ5400jtA3yqr-5frSaE0N9lifir1UzbDkXviKCVq6Nj6V86EdzfIQ_vS1cedWl-drw_fn48DQDw_MlrmAIzLCr8Tv9howmONQUkwjhvc5Cd_9l3mutzr3ACYJzkIaCgYKAXUSARMSFQHGX2Mi9rfaw6NRcaajB77iZdv7uw0183\"}\n",
        "    response = requests.post(api_url, headers=headers, json=payload)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        return response.json()\n",
        "    else:\n",
        "        raise Exception(f\"Error: {response.status_code}, {response.text}\")\n",
        "\n",
        "# Call the OCR function and get the data\n",
        "ocr_response = get_ocr_data(pdf_path)\n",
        "\n",
        "def to_camel_case(snake_str):\n",
        "    \"\"\"Convert snake_case to camelCase.\"\"\"\n",
        "    components = snake_str.lower().split('_')\n",
        "    return components[0] + ''.join(x.title() for x in components[1:])\n",
        "\n",
        "def find_word_indices(content, word):\n",
        "    \"\"\"Find the start and end indices of a word in the digitized content, ignoring case differences.\"\"\"\n",
        "    content_lower = content.lower()\n",
        "    word_lower = word.lower()\n",
        "    start_index = content_lower.find(word_lower)\n",
        "    if start_index == -1:\n",
        "        return None  # Word not found\n",
        "    end_index = start_index + len(word)\n",
        "    return start_index, end_index\n",
        "\n",
        "def map_kili_to_gcp(kili_response, digitized_content, ocr_response):\n",
        "    \"\"\"Map Kili annotations to GCP-compatible JSON format.\"\"\"\n",
        "    gcp_response = {\n",
        "        \"entities\": [],\n",
        "        \"pages\": [],\n",
        "        \"shardInfo\": {\n",
        "            \"shardCount\": \"1\"\n",
        "        },\n",
        "        \"text\": ocr_response['document']['text'],\n",
        "        \"uri\": \"\"  # Assuming this is a placeholder, adjust if necessary\n",
        "    }\n",
        "\n",
        "    # Add pages to GCP response\n",
        "    for page in ocr_response['document']['pages']:\n",
        "        gcp_response[\"pages\"].append(page)\n",
        "\n",
        "    # Debugging: Check structure of Kili JSON\n",
        "    if kili_response and isinstance(kili_response, list):\n",
        "        kili_json_response = kili_response[0].get(\"latestLabel\", {}).get(\"jsonResponse\", {})\n",
        "        # print(\"Keys in jsonResponse:\", list(kili_json_response.keys()))  # Debugging\n",
        "\n",
        "        for job_key, job_value in kili_json_response.items():\n",
        "            if \"annotations\" in job_value:\n",
        "                annotations = job_value[\"annotations\"]\n",
        "                # print(f\"Found {len(annotations)} annotations under {job_key}\")  # Debugging\n",
        "\n",
        "                for annotation_entry in annotations:\n",
        "                    for annotation in annotation_entry.get(\"annotations\", []):\n",
        "                        # Debugging: Print each annotation\n",
        "                        # print(f\"Annotation: {annotation}\")\n",
        "\n",
        "                        # Extract mention text and category\n",
        "                        mention_text = annotation_entry.get(\"content\", \"\").upper()\n",
        "                        category = annotation_entry.get(\"categories\", [{}])[0].get(\"name\", \"UNKNOWN\")\n",
        "\n",
        "                        # Convert category name to camelCase for GCP \"type\"\n",
        "                        entity_type = to_camel_case(category)\n",
        "\n",
        "                        # Generate a unique ID for each entity\n",
        "                        entity_id = str(uuid.uuid4())[:16]\n",
        "\n",
        "                        # Find the start and end indices based on the digitized content\n",
        "                        indices = find_word_indices(digitized_content, mention_text)\n",
        "\n",
        "                        if indices is None:\n",
        "                            print(f\"Warning: '{mention_text}' not found in digitized content.\")\n",
        "                            start_index, end_index = \"\", \"\"\n",
        "                        else:\n",
        "                            start_index, end_index = indices\n",
        "\n",
        "                        # Map boundingPoly and normalizedVertices\n",
        "                        bounding_poly = annotation.get(\"boundingPoly\",[{}])[0].get(\"normalizedVertices\",[{}])\n",
        "\n",
        "                        gcp_entity = {\n",
        "                            \"confidence\":1,\n",
        "                            \"id\": entity_id,\n",
        "                            \"mentionText\": mention_text,\n",
        "                            \"pageAnchor\": {\n",
        "                                \"pageRefs\": [\n",
        "                                    {\n",
        "                                        \"boundingPoly\": {\n",
        "                                            \"normalizedVertices\": bounding_poly\n",
        "                                        },\n",
        "                                        \"layoutType\": \"VISUAL_ELEMENT\"\n",
        "                                    }\n",
        "                                ]\n",
        "                            },\n",
        "                            \"textAnchor\": {\n",
        "                                \"content\": mention_text + \"\\n\",\n",
        "                                \"textSegments\": [\n",
        "                                    {\n",
        "                                        \"endIndex\": str(end_index) if end_index else \"\",\n",
        "                                        \"startIndex\": str(start_index) if start_index else \"\"\n",
        "                                    }\n",
        "                                ]\n",
        "                            },\n",
        "                            \"type\": entity_type\n",
        "                        }\n",
        "                        gcp_response[\"entities\"].append(gcp_entity)\n",
        "\n",
        "    return gcp_response\n",
        "\n",
        "def flatten_normalized_vertices(data):\n",
        "    \"\"\"\n",
        "    Recursively processes the JSON to flatten nested normalizedVertices arrays.\n",
        "    \"\"\"\n",
        "    if isinstance(data, list):\n",
        "        return [flatten_normalized_vertices(item) for item in data]\n",
        "\n",
        "    if isinstance(data, dict):\n",
        "        new_dict = {}\n",
        "        for key, value in data.items():\n",
        "            if key == 'normalizedVertices' and isinstance(value, list) and len(value) > 0:\n",
        "                # Remove one level of nesting if it exists\n",
        "                new_dict[key] = value[0] if isinstance(value[0], list) else value\n",
        "            else:\n",
        "                new_dict[key] = flatten_normalized_vertices(value)\n",
        "        return new_dict\n",
        "\n",
        "    return data\n",
        "\n",
        "def process_kili_json(kili_json):\n",
        "    # Create a deep copy to avoid modifying the original data\n",
        "    modified_json = copy.deepcopy(kili_json)\n",
        "    return flatten_normalized_vertices(modified_json)\n",
        "\n",
        "# Open the JSON file\n",
        "with open('kili_resume_sample.json', 'r') as file:\n",
        "    # Load the JSON content\n",
        "    kili_json = json.load(file)\n",
        "\n",
        "digitized_content = ocr_response['document']['text']\n",
        "#print(digitized_content)\n",
        "\n",
        "# Example usage\n",
        "# Assuming kili_json is your input JSON\n",
        "modified_data = process_kili_json(kili_json)\n",
        "\n",
        "# Map Kili annotations to GCP format\n",
        "gcp_json = map_kili_to_gcp(modified_data, digitized_content,ocr_response)\n",
        "\n",
        "# Save to a JSON file\n",
        "output_file = \"python_pdf_kili_to_gcp_resume_01.json\"\n",
        "with open(output_file, \"w\", encoding=\"utf-8\") as file:\n",
        "    json.dump(gcp_json, file, ensure_ascii=False, indent=4)\n",
        "\n",
        "print(f\"GCP JSON saved to {output_file}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oyYQ-lymCpLP",
        "outputId": "970477cc-1972-44f5-b27c-d5b19df1ddb4"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GCP JSON saved to python_pdf_kili_to_gcp_resume_01.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Resume with multiple lables within a label"
      ],
      "metadata": {
        "id": "y-HhezklXWpA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import uuid\n",
        "import json\n",
        "import base64\n",
        "import requests\n",
        "import copy\n",
        "\n",
        "# Define the PDF file path\n",
        "pdf_path = \"/content/CriminalJusticeResumedocx.pdf\"\n",
        "\n",
        "# Function to send PDF for OCR processing\n",
        "def get_ocr_data(pdf_path):\n",
        "    # Google Document AI endpoint\n",
        "    api_url = \"https://us-documentai.googleapis.com/v1/projects/205398200267/locations/us/processors/2dde9d14469e70c9:process\"\n",
        "\n",
        "    # Read the PDF content and encode it in base64\n",
        "    with open(pdf_path, \"rb\") as pdf_file:\n",
        "        pdf_content = pdf_file.read()\n",
        "\n",
        "    # Convert PDF to base64 (this is required for most APIs)\n",
        "    encoded_pdf = base64.b64encode(pdf_content).decode(\"utf-8\")\n",
        "\n",
        "    # API request payload (adjusted structure for Document AI)\n",
        "    payload = {\n",
        "        \"skipHumanReview\": True,\n",
        "        \"inlineDocument\": {\n",
        "            \"mimeType\": \"application/pdf\",\n",
        "            \"content\": encoded_pdf\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # Make the API call (ensure you have an API key or OAuth token)\n",
        "    headers = {\"Authorization\": \"Bearer ya29.a0ARW5m76KwZoNmymB7fowECoEy3mB4vvcO-7fRzLhPFSKjvKAJbcpreO8-6sx-GJ9FTzpMLrpIYfbvOAJnxWXwfSAxVfbtPKXaIluYRhhszfQLl0G4HmADmxhCjPyGrohu6K1bBA9T-_rfyi7krZQoUZx_FO6oLyRDtvfDGIOFk2Vn6fMaCgYKATESARMSFQHGX2MiUSa37Xqc0w-seT0O5rfwDg0183\"}\n",
        "    response = requests.post(api_url, headers=headers, json=payload)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        return response.json()\n",
        "    else:\n",
        "        raise Exception(f\"Error: {response.status_code}, {response.text}\")\n",
        "\n",
        "# Call the OCR function and get the data\n",
        "ocr_response = get_ocr_data(pdf_path)\n",
        "\n",
        "def to_camel_case(snake_str):\n",
        "    \"\"\"Convert snake_case to camelCase.\"\"\"\n",
        "    components = snake_str.lower().split('_')\n",
        "    return components[0] + ''.join(x.title() for x in components[1:])\n",
        "\n",
        "def find_word_indices(content, word):\n",
        "    \"\"\"Find the start and end indices of a word in the digitized content, ignoring case differences.\"\"\"\n",
        "    content_lower = content.lower()\n",
        "    word_lower = word.lower()\n",
        "    start_index = content_lower.find(word_lower)\n",
        "    if start_index == -1:\n",
        "        return None  # Word not found\n",
        "    end_index = start_index + len(word)\n",
        "    return start_index, end_index\n",
        "\n",
        "def map_kili_to_gcp(kili_response, digitized_content, ocr_response):\n",
        "    \"\"\"Map Kili annotations to GCP-compatible JSON format.\"\"\"\n",
        "    gcp_response = {\n",
        "        \"entities\": [],\n",
        "        \"pages\": [],\n",
        "        \"shardInfo\": {\n",
        "            \"shardCount\": \"1\"\n",
        "        },\n",
        "        \"text\": ocr_response['document']['text'],\n",
        "        \"uri\": \"\"  # Assuming this is a placeholder, adjust if necessary\n",
        "    }\n",
        "\n",
        "    # Add pages to GCP response\n",
        "    for page in ocr_response['document']['pages']:\n",
        "        gcp_response[\"pages\"].append(page)\n",
        "\n",
        "    # Debugging: Check structure of Kili JSON\n",
        "    if kili_response and isinstance(kili_response, list):\n",
        "        kili_json_response = kili_response[0].get(\"latestLabel\", {}).get(\"jsonResponse\", {})\n",
        "        # print(\"Keys in jsonResponse:\", list(kili_json_response.keys()))  # Debugging\n",
        "\n",
        "        for job_key, job_value in kili_json_response.items():\n",
        "            if \"annotations\" in job_value:\n",
        "                annotations = job_value[\"annotations\"]\n",
        "                # print(f\"Found {len(annotations)} annotations under {job_key}\")  # Debugging\n",
        "\n",
        "                for annotation_entry in annotations:\n",
        "                    for annotation in annotation_entry.get(\"annotations\", []):\n",
        "                        # Debugging: Print each annotation\n",
        "                        # print(f\"Annotation: {annotation}\")\n",
        "\n",
        "                        # Extract mention text and category\n",
        "                        mention_text = annotation_entry.get(\"content\", \"\").upper()\n",
        "                        category = annotation_entry.get(\"categories\", [{}])[0].get(\"name\", \"UNKNOWN\")\n",
        "\n",
        "                        # Convert category name to camelCase for GCP \"type\"\n",
        "                        entity_type = to_camel_case(category)\n",
        "\n",
        "                        # Generate a unique ID for each entity\n",
        "                        entity_id = str(uuid.uuid4())[:16]\n",
        "\n",
        "                        # Find the start and end indices based on the digitized content\n",
        "                        indices = find_word_indices(digitized_content, mention_text)\n",
        "\n",
        "                        if indices is None:\n",
        "                            print(f\"Warning: '{mention_text}' not found in digitized content.\")\n",
        "                            start_index, end_index = \"\", \"\"\n",
        "                        else:\n",
        "                            start_index, end_index = indices\n",
        "\n",
        "                        # Map boundingPoly and normalizedVertices\n",
        "                        bounding_poly = annotation.get(\"boundingPoly\",[{}])[0].get(\"normalizedVertices\",[{}])\n",
        "\n",
        "                        gcp_entity = {\n",
        "                            \"confidence\":1,\n",
        "                            \"id\": entity_id,\n",
        "                            \"mentionText\": mention_text,\n",
        "                            \"pageAnchor\": {\n",
        "                                \"pageRefs\": [\n",
        "                                    {\n",
        "                                        \"boundingPoly\": {\n",
        "                                            \"normalizedVertices\": bounding_poly\n",
        "                                        },\n",
        "                                        \"layoutType\": \"VISUAL_ELEMENT\"\n",
        "                                    }\n",
        "                                ]\n",
        "                            },\n",
        "                            \"textAnchor\": {\n",
        "                                \"content\": mention_text + \"\\n\",\n",
        "                                \"textSegments\": [\n",
        "                                    {\n",
        "                                        \"endIndex\": str(end_index) if end_index else \"\",\n",
        "                                        \"startIndex\": str(start_index) if start_index else \"\"\n",
        "                                    }\n",
        "                                ]\n",
        "                            },\n",
        "                            \"type\": entity_type\n",
        "                        }\n",
        "                        gcp_response[\"entities\"].append(gcp_entity)\n",
        "\n",
        "    return gcp_response\n",
        "\n",
        "def flatten_normalized_vertices(data):\n",
        "    \"\"\"\n",
        "    Recursively processes the JSON to flatten nested normalizedVertices arrays.\n",
        "    \"\"\"\n",
        "    if isinstance(data, list):\n",
        "        return [flatten_normalized_vertices(item) for item in data]\n",
        "\n",
        "    if isinstance(data, dict):\n",
        "        new_dict = {}\n",
        "        for key, value in data.items():\n",
        "            if key == 'normalizedVertices' and isinstance(value, list) and len(value) > 0:\n",
        "                # Remove one level of nesting if it exists\n",
        "                new_dict[key] = value[0] if isinstance(value[0], list) else value\n",
        "            else:\n",
        "                new_dict[key] = flatten_normalized_vertices(value)\n",
        "        return new_dict\n",
        "\n",
        "    return data\n",
        "\n",
        "def process_kili_json(kili_json):\n",
        "    # Create a deep copy to avoid modifying the original data\n",
        "    modified_json = copy.deepcopy(kili_json)\n",
        "    return flatten_normalized_vertices(modified_json)\n",
        "\n",
        "# Open the JSON file\n",
        "with open('kili_sublable.json', 'r') as file:\n",
        "    # Load the JSON content\n",
        "    kili_json = json.load(file)\n",
        "\n",
        "digitized_content = ocr_response['document']['text']\n",
        "#print(digitized_content)\n",
        "\n",
        "# Example usage\n",
        "# Assuming kili_json is your input JSON\n",
        "modified_data = process_kili_json(kili_json)\n",
        "\n",
        "# Map Kili annotations to GCP format\n",
        "gcp_json = map_kili_to_gcp(modified_data, digitized_content,ocr_response)\n",
        "\n",
        "# Save to a JSON file\n",
        "output_file = \"python_pdf_kili_to_gcp_resume_07.json\"\n",
        "with open(output_file, \"w\", encoding=\"utf-8\") as file:\n",
        "    json.dump(gcp_json, file, ensure_ascii=False, indent=4)\n",
        "\n",
        "print(f\"GCP JSON saved to {output_file}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VvHMsJxTXc2L",
        "outputId": "927457a6-fc18-4aab-9b63-85bd4e20bc80"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GCP JSON saved to python_pdf_kili_to_gcp_resume_07.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Resume with multiple lines"
      ],
      "metadata": {
        "id": "niDq2GJNc2Yk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import uuid\n",
        "import json\n",
        "import base64\n",
        "import requests\n",
        "import copy\n",
        "\n",
        "# Define the PDF file path\n",
        "pdf_path = \"/content/CriminalJusticeResumedocx.pdf\"\n",
        "\n",
        "# Function to send PDF for OCR processing\n",
        "def get_ocr_data(pdf_path):\n",
        "    # Google Document AI endpoint\n",
        "    api_url = \"https://us-documentai.googleapis.com/v1/projects/205398200267/locations/us/processors/2dde9d14469e70c9:process\"\n",
        "\n",
        "    # Read the PDF content and encode it in base64\n",
        "    with open(pdf_path, \"rb\") as pdf_file:\n",
        "        pdf_content = pdf_file.read()\n",
        "\n",
        "    # Convert PDF to base64 (this is required for most APIs)\n",
        "    encoded_pdf = base64.b64encode(pdf_content).decode(\"utf-8\")\n",
        "\n",
        "    # API request payload (adjusted structure for Document AI)\n",
        "    payload = {\n",
        "        \"skipHumanReview\": True,\n",
        "        \"inlineDocument\": {\n",
        "            \"mimeType\": \"application/pdf\",\n",
        "            \"content\": encoded_pdf\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # Make the API call (ensure you have an API key or OAuth token)\n",
        "    headers = {\"Authorization\": \"Bearer ya29.a0ARW5m76KwZoNmymB7fowECoEy3mB4vvcO-7fRzLhPFSKjvKAJbcpreO8-6sx-GJ9FTzpMLrpIYfbvOAJnxWXwfSAxVfbtPKXaIluYRhhszfQLl0G4HmADmxhCjPyGrohu6K1bBA9T-_rfyi7krZQoUZx_FO6oLyRDtvfDGIOFk2Vn6fMaCgYKATESARMSFQHGX2MiUSa37Xqc0w-seT0O5rfwDg0183\"}\n",
        "    response = requests.post(api_url, headers=headers, json=payload)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        return response.json()\n",
        "    else:\n",
        "        raise Exception(f\"Error: {response.status_code}, {response.text}\")\n",
        "\n",
        "# Call the OCR function and get the data\n",
        "ocr_response = get_ocr_data(pdf_path)\n",
        "\n",
        "def to_camel_case(snake_str):\n",
        "    \"\"\"Convert snake_case to camelCase.\"\"\"\n",
        "    components = snake_str.lower().split('_')\n",
        "    return components[0] + ''.join(x.title() for x in components[1:])\n",
        "\n",
        "def find_word_indices(content, word):\n",
        "    \"\"\"Find the start and end indices of a word in the digitized content, ignoring case differences.\"\"\"\n",
        "    content_lower = content.lower()\n",
        "    word_lower = word.lower()\n",
        "    start_index = content_lower.find(word_lower)\n",
        "    if start_index == -1:\n",
        "        return None  # Word not found\n",
        "    end_index = start_index + len(word)\n",
        "    return start_index, end_index\n",
        "\n",
        "def map_kili_to_gcp(kili_response, digitized_content, ocr_response):\n",
        "    \"\"\"Map Kili annotations to GCP-compatible JSON format.\"\"\"\n",
        "    gcp_response = {\n",
        "        \"entities\": [],\n",
        "        \"pages\": [],\n",
        "        \"shardInfo\": {\n",
        "            \"shardCount\": \"1\"\n",
        "        },\n",
        "        \"text\": ocr_response['document']['text'],\n",
        "        \"uri\": \"\"  # Assuming this is a placeholder, adjust if necessary\n",
        "    }\n",
        "\n",
        "    # Add pages to GCP response\n",
        "    for page in ocr_response['document']['pages']:\n",
        "        gcp_response[\"pages\"].append(page)\n",
        "\n",
        "    # Debugging: Check structure of Kili JSON\n",
        "    if kili_response and isinstance(kili_response, list):\n",
        "        kili_json_response = kili_response[0].get(\"latestLabel\", {}).get(\"jsonResponse\", {})\n",
        "        # print(\"Keys in jsonResponse:\", list(kili_json_response.keys()))  # Debugging\n",
        "\n",
        "        for job_key, job_value in kili_json_response.items():\n",
        "            if \"annotations\" in job_value:\n",
        "                annotations = job_value[\"annotations\"]\n",
        "                # print(f\"Found {len(annotations)} annotations under {job_key}\")  # Debugging\n",
        "\n",
        "                for annotation_entry in annotations:\n",
        "                    for annotation in annotation_entry.get(\"annotations\", []):\n",
        "                        # Debugging: Print each annotation\n",
        "                        # print(f\"Annotation: {annotation}\")\n",
        "\n",
        "                        # Extract mention text and category\n",
        "                        mention_text = annotation_entry.get(\"content\", \"\").upper()\n",
        "                        category = annotation_entry.get(\"categories\", [{}])[0].get(\"name\", \"UNKNOWN\")\n",
        "\n",
        "                        # Convert category name to camelCase for GCP \"type\"\n",
        "                        entity_type = to_camel_case(category)\n",
        "\n",
        "                        # Generate a unique ID for each entity\n",
        "                        entity_id = str(uuid.uuid4())[:16]\n",
        "\n",
        "                        # Find the start and end indices based on the digitized content\n",
        "                        indices = find_word_indices(digitized_content, mention_text)\n",
        "\n",
        "                        if indices is None:\n",
        "                            print(f\"Warning: '{mention_text}' not found in digitized content.\")\n",
        "                            start_index, end_index = \"\", \"\"\n",
        "                        else:\n",
        "                            start_index, end_index = indices\n",
        "\n",
        "                        # Map boundingPoly and normalizedVertices\n",
        "                        bounding_poly = annotation.get(\"boundingPoly\",[{}])[0].get(\"normalizedVertices\",[{}])\n",
        "\n",
        "                        gcp_entity = {\n",
        "                            \"confidence\":1,\n",
        "                            \"id\": entity_id,\n",
        "                            \"mentionText\": mention_text,\n",
        "                            \"pageAnchor\": {\n",
        "                                \"pageRefs\": [\n",
        "                                    {\n",
        "                                        \"boundingPoly\": {\n",
        "                                            \"normalizedVertices\": bounding_poly\n",
        "                                        },\n",
        "                                        \"layoutType\": \"VISUAL_ELEMENT\"\n",
        "                                    }\n",
        "                                ]\n",
        "                            },\n",
        "                            \"textAnchor\": {\n",
        "                                \"content\": mention_text + \"\\n\",\n",
        "                                \"textSegments\": [\n",
        "                                    {\n",
        "                                        \"endIndex\": str(end_index) if end_index else \"\",\n",
        "                                        \"startIndex\": str(start_index) if start_index else \"\"\n",
        "                                    }\n",
        "                                ]\n",
        "                            },\n",
        "                            \"type\": entity_type\n",
        "                        }\n",
        "                        gcp_response[\"entities\"].append(gcp_entity)\n",
        "\n",
        "    return gcp_response\n",
        "\n",
        "def flatten_normalized_vertices(data):\n",
        "    \"\"\"\n",
        "    Recursively processes the JSON to flatten nested normalizedVertices arrays.\n",
        "    \"\"\"\n",
        "    if isinstance(data, list):\n",
        "        return [flatten_normalized_vertices(item) for item in data]\n",
        "\n",
        "    if isinstance(data, dict):\n",
        "        new_dict = {}\n",
        "        for key, value in data.items():\n",
        "            if key == 'normalizedVertices' and isinstance(value, list) and len(value) > 0:\n",
        "                # Remove one level of nesting if it exists\n",
        "                new_dict[key] = value[0] if isinstance(value[0], list) else value\n",
        "            else:\n",
        "                new_dict[key] = flatten_normalized_vertices(value)\n",
        "        return new_dict\n",
        "\n",
        "    return data\n",
        "\n",
        "def process_kili_json(kili_json):\n",
        "    # Create a deep copy to avoid modifying the original data\n",
        "    modified_json = copy.deepcopy(kili_json)\n",
        "    return flatten_normalized_vertices(modified_json)\n",
        "\n",
        "# Open the JSON file\n",
        "with open('kili_multiple_lines.json', 'r') as file:\n",
        "    # Load the JSON content\n",
        "    kili_json = json.load(file)\n",
        "\n",
        "digitized_content = ocr_response['document']['text']\n",
        "#print(digitized_content)\n",
        "\n",
        "# Example usage\n",
        "# Assuming kili_json is your input JSON\n",
        "modified_data = process_kili_json(kili_json)\n",
        "\n",
        "# Map Kili annotations to GCP format\n",
        "gcp_json = map_kili_to_gcp(modified_data, digitized_content,ocr_response)\n",
        "\n",
        "# Save to a JSON file\n",
        "output_file = \"python_pdf_kili_to_gcp_resume_08.json\"\n",
        "with open(output_file, \"w\", encoding=\"utf-8\") as file:\n",
        "    json.dump(gcp_json, file, ensure_ascii=False, indent=4)\n",
        "\n",
        "print(f\"GCP JSON saved to {output_file}\")"
      ],
      "metadata": {
        "id": "ANEKx1ZRcxQx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import uuid\n",
        "import json\n",
        "import base64\n",
        "import requests\n",
        "import copy\n",
        "import re\n",
        "\n",
        "# Define the PDF file path\n",
        "pdf_path = \"/content/CriminalJusticeResumedocx.pdf\"\n",
        "\n",
        "# Function to send PDF for OCR processing\n",
        "def get_ocr_data(pdf_path):\n",
        "    # Google Document AI endpoint\n",
        "    api_url = \"https://us-documentai.googleapis.com/v1/projects/205398200267/locations/us/processors/2dde9d14469e70c9:process\"\n",
        "\n",
        "    # Read the PDF content and encode it in base64\n",
        "    with open(pdf_path, \"rb\") as pdf_file:\n",
        "        pdf_content = pdf_file.read()\n",
        "\n",
        "    # Convert PDF to base64 (this is required for most APIs)\n",
        "    encoded_pdf = base64.b64encode(pdf_content).decode(\"utf-8\")\n",
        "\n",
        "    # API request payload (adjusted structure for Document AI)\n",
        "    payload = {\n",
        "        \"skipHumanReview\": True,\n",
        "        \"inlineDocument\": {\n",
        "            \"mimeType\": \"application/pdf\",\n",
        "            \"content\": encoded_pdf\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # Make the API call (ensure you have an API key or OAuth token)\n",
        "    headers = {\"Authorization\": \"Bearer ya29.a0ARW5m76KwZoNmymB7fowECoEy3mB4vvcO-7fRzLhPFSKjvKAJbcpreO8-6sx-GJ9FTzpMLrpIYfbvOAJnxWXwfSAxVfbtPKXaIluYRhhszfQLl0G4HmADmxhCjPyGrohu6K1bBA9T-_rfyi7krZQoUZx_FO6oLyRDtvfDGIOFk2Vn6fMaCgYKATESARMSFQHGX2MiUSa37Xqc0w-seT0O5rfwDg0183\"}\n",
        "    response = requests.post(api_url, headers=headers, json=payload)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        return response.json()\n",
        "    else:\n",
        "        raise Exception(f\"Error: {response.status_code}, {response.text}\")\n",
        "\n",
        "# Call the OCR function and get the data\n",
        "ocr_response = get_ocr_data(pdf_path)\n",
        "\n",
        "def to_camel_case(snake_str):\n",
        "    \"\"\"Convert snake_case to camelCase.\"\"\"\n",
        "    components = snake_str.lower().split('_')\n",
        "    return components[0] + ''.join(x.title() for x in components[1:])\n",
        "\n",
        "def normalize_text(text):\n",
        "    \"\"\"\n",
        "    Normalize text by:\n",
        "    1. Converting to uppercase\n",
        "    2. Removing extra whitespace\n",
        "    3. Removing special characters\n",
        "    4. Standardizing line breaks\n",
        "    \"\"\"\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "\n",
        "    # Convert to uppercase\n",
        "    text = text.upper()\n",
        "    # Replace multiple spaces and newlines with single space\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    # Remove special characters but keep basic punctuation\n",
        "    text = re.sub(r'[^\\w\\s.,●-]', '', text)\n",
        "    # Strip leading/trailing whitespace\n",
        "    text = text.strip()\n",
        "    return text\n",
        "\n",
        "def find_word_indices(content, word):\n",
        "    \"\"\"\n",
        "    Find the start and end indices of a word in the digitized content,\n",
        "    using normalized versions of both texts for comparison.\n",
        "    \"\"\"\n",
        "    if not word or not content:\n",
        "        return None\n",
        "\n",
        "    # Normalize both the content and the search word\n",
        "    normalized_content = normalize_text(content)\n",
        "    normalized_word = normalize_text(word)\n",
        "\n",
        "    # First try exact match\n",
        "    start_index = normalized_content.find(normalized_word)\n",
        "\n",
        "    if start_index == -1:\n",
        "        # If exact match fails, try matching parts\n",
        "        words = normalized_word.split()\n",
        "        for i in range(len(words)):\n",
        "            # Try matching with fewer words\n",
        "            partial_phrase = ' '.join(words[:i+1])\n",
        "            if partial_phrase in normalized_content:\n",
        "                start_index = normalized_content.find(partial_phrase)\n",
        "                end_index = start_index + len(partial_phrase)\n",
        "                # Map back to original content indices\n",
        "                original_start = len(normalize_text(content[:start_index]))\n",
        "                original_end = original_start + len(word)\n",
        "                return original_start, original_end\n",
        "        return None\n",
        "\n",
        "    end_index = start_index + len(normalized_word)\n",
        "\n",
        "    # Map back to original content indices\n",
        "    original_start = len(normalize_text(content[:start_index]))\n",
        "    original_end = original_start + len(word)\n",
        "    return original_start, original_end\n",
        "\n",
        "def map_kili_to_gcp(kili_response, digitized_content, ocr_response):\n",
        "    \"\"\"Map Kili annotations to GCP-compatible JSON format.\"\"\"\n",
        "    gcp_response = {\n",
        "        \"entities\": [],\n",
        "        \"pages\": [],\n",
        "        \"shardInfo\": {\n",
        "            \"shardCount\": \"1\"\n",
        "        },\n",
        "        \"text\": ocr_response['document']['text'],\n",
        "        \"uri\": \"\"\n",
        "    }\n",
        "\n",
        "    # Add pages to GCP response\n",
        "    for page in ocr_response['document']['pages']:\n",
        "        gcp_response[\"pages\"].append(page)\n",
        "\n",
        "    if kili_response and isinstance(kili_response, list):\n",
        "        kili_json_response = kili_response[0].get(\"latestLabel\", {}).get(\"jsonResponse\", {})\n",
        "\n",
        "        for job_key, job_value in kili_json_response.items():\n",
        "            if \"annotations\" in job_value:\n",
        "                annotations = job_value[\"annotations\"]\n",
        "\n",
        "                for annotation_entry in annotations:\n",
        "                    for annotation in annotation_entry.get(\"annotations\", []):\n",
        "                        mention_text = annotation_entry.get(\"content\", \"\")\n",
        "                        category = annotation_entry.get(\"categories\", [{}])[0].get(\"name\", \"UNKNOWN\")\n",
        "\n",
        "                        # Convert category name to camelCase for GCP \"type\"\n",
        "                        entity_type = to_camel_case(category)\n",
        "\n",
        "                        # Generate a unique ID for each entity\n",
        "                        entity_id = str(uuid.uuid4())[:16]\n",
        "\n",
        "                        # Find the start and end indices based on the digitized content\n",
        "                        indices = find_word_indices(digitized_content, mention_text)\n",
        "\n",
        "                        if indices is None:\n",
        "                            print(f\"Warning: Could not find exact match for: '{mention_text}'\")\n",
        "                            print(f\"Normalized version: '{normalize_text(mention_text)}'\")\n",
        "                            start_index, end_index = 0, len(mention_text)  # Fallback values\n",
        "                        else:\n",
        "                            start_index, end_index = indices\n",
        "\n",
        "                        # Map boundingPoly and normalizedVertices\n",
        "                        bounding_poly = annotation.get(\"boundingPoly\", [{}])[0].get(\"normalizedVertices\", [{}])\n",
        "\n",
        "                        gcp_entity = {\n",
        "                            \"confidence\": 1,\n",
        "                            \"id\": entity_id,\n",
        "                            \"mentionText\": mention_text,\n",
        "                            \"pageAnchor\": {\n",
        "                                \"pageRefs\": [\n",
        "                                    {\n",
        "                                        \"boundingPoly\": {\n",
        "                                            \"normalizedVertices\": bounding_poly\n",
        "                                        },\n",
        "                                        \"layoutType\": \"VISUAL_ELEMENT\"\n",
        "                                    }\n",
        "                                ]\n",
        "                            },\n",
        "                            \"textAnchor\": {\n",
        "                                \"content\": mention_text + \"\\n\",\n",
        "                                \"textSegments\": [\n",
        "                                    {\n",
        "                                        \"endIndex\": str(end_index),\n",
        "                                        \"startIndex\": str(start_index)\n",
        "                                    }\n",
        "                                ]\n",
        "                            },\n",
        "                            \"type\": entity_type\n",
        "                        }\n",
        "                        gcp_response[\"entities\"].append(gcp_entity)\n",
        "\n",
        "    return gcp_response\n",
        "\n",
        "def flatten_normalized_vertices(data):\n",
        "    \"\"\"\n",
        "    Recursively processes the JSON to flatten nested normalizedVertices arrays.\n",
        "    \"\"\"\n",
        "    if isinstance(data, list):\n",
        "        return [flatten_normalized_vertices(item) for item in data]\n",
        "\n",
        "    if isinstance(data, dict):\n",
        "        new_dict = {}\n",
        "        for key, value in data.items():\n",
        "            if key == 'normalizedVertices' and isinstance(value, list) and len(value) > 0:\n",
        "                # Remove one level of nesting if it exists\n",
        "                new_dict[key] = value[0] if isinstance(value[0], list) else value\n",
        "            else:\n",
        "                new_dict[key] = flatten_normalized_vertices(value)\n",
        "        return new_dict\n",
        "\n",
        "    return data\n",
        "\n",
        "def process_kili_json(kili_json):\n",
        "    # Create a deep copy to avoid modifying the original data\n",
        "    modified_json = copy.deepcopy(kili_json)\n",
        "    return flatten_normalized_vertices(modified_json)\n",
        "\n",
        "# Open the JSON file\n",
        "with open('final_kili_check.json', 'r') as file:\n",
        "    # Load the JSON content\n",
        "    kili_json = json.load(file)\n",
        "\n",
        "digitized_content = ocr_response['document']['text']\n",
        "#print(digitized_content)\n",
        "\n",
        "# Example usage\n",
        "# Assuming kili_json is your input JSON\n",
        "modified_data = process_kili_json(kili_json)\n",
        "\n",
        "# Map Kili annotations to GCP format\n",
        "gcp_json = map_kili_to_gcp(modified_data, digitized_content,ocr_response)\n",
        "\n",
        "# Save to a JSON file\n",
        "output_file = \"python_pdf_kili_to_gcp_resume_10.json\"\n",
        "with open(output_file, \"w\", encoding=\"utf-8\") as file:\n",
        "    json.dump(gcp_json, file, ensure_ascii=False, indent=4)\n",
        "\n",
        "print(f\"GCP JSON saved to {output_file}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "52ZOlfu2hcOA",
        "outputId": "c4f7d09d-d7a7-4f2c-d17d-d43880bd720f"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GCP JSON saved to python_pdf_kili_to_gcp_resume_10.json\n"
          ]
        }
      ]
    }
  ]
}